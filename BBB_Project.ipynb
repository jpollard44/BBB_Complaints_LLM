{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f84c60031af349dc8a4b0abaf99b6343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7b85a9dc24c4a0f9567723f27614f7a",
              "IPY_MODEL_0058dd4a235546fc9a9d54fc3f09325c",
              "IPY_MODEL_0b4795ab34944dd5b8ad529c843ba34a"
            ],
            "layout": "IPY_MODEL_30a790cf7f214e2e97eb9fdd7f46c566"
          }
        },
        "d7b85a9dc24c4a0f9567723f27614f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a2adc15cf7a4e3ab8adf48dc5fd0805",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e1b9990b5b4b4992a96c1928264498f3",
            "value": "Batches:‚Äá100%"
          }
        },
        "0058dd4a235546fc9a9d54fc3f09325c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ec27afaf780493fb7eaca432590f55e",
            "max": 113,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8689874f71e3435794e450053a251b2a",
            "value": 113
          }
        },
        "0b4795ab34944dd5b8ad529c843ba34a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e159ca3f79645c5870a6a1c0e4f66e2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_57321e4bd029481bb9437d8a3d89cf33",
            "value": "‚Äá113/113‚Äá[03:39&lt;00:00,‚Äá‚Äá2.96it/s]"
          }
        },
        "30a790cf7f214e2e97eb9fdd7f46c566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2adc15cf7a4e3ab8adf48dc5fd0805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1b9990b5b4b4992a96c1928264498f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ec27afaf780493fb7eaca432590f55e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8689874f71e3435794e450053a251b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e159ca3f79645c5870a6a1c0e4f66e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57321e4bd029481bb9437d8a3d89cf33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpollard44/BBB_Complaints_LLM/blob/main/BBB_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code for Data scraper**"
      ],
      "metadata": {
        "id": "VFA8YV2KlYrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ],
      "metadata": {
        "id": "thGfOB41lMwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "a03a61d0-60da-42f4-ff81-c8826978fcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'undetected_chromedriver'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3994371303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mundetected_chromedriver\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0muc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'undetected_chromedriver'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to scrape random pages of BBB complaints ---\n",
        "def scrape_bbb_complaints_random(base_url, min_page=1, max_page=200, num_samples=20, pause=1.0, timeout=10):\n",
        "    \"\"\"\n",
        "    Scrape a random sample of complaint pages from BBB\n",
        "    across a range of pages to diversify by time.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Set up headless Chrome browser using undetected-chromedriver\n",
        "    options = uc.ChromeOptions()\n",
        "    options.headless = False  # Set to True if you want headless mode\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Avoid bot detection\n",
        "    options.add_argument(\n",
        "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/116.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "    driver = uc.Chrome(options=options)\n",
        "\n",
        "    # Remove webdriver detection flag from JavaScript environment\n",
        "    driver.execute_cdp_cmd(\n",
        "        \"Page.addScriptToEvaluateOnNewDocument\",\n",
        "        {\n",
        "            \"source\": \"\"\"\n",
        "                Object.defineProperty(navigator, 'webdriver', {\n",
        "                  get: () => undefined\n",
        "                });\n",
        "            \"\"\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 2) Randomly choose a set of pages to scrape\n",
        "    pages_to_scrape = sorted(random.sample(range(min_page, max_page + 1), num_samples))\n",
        "    print(f\"üîÄ Randomly selected pages: {pages_to_scrape}\")\n",
        "\n",
        "    all_rows = []  # List to collect complaint data\n",
        "\n",
        "    for page in pages_to_scrape:\n",
        "        url = f\"{base_url}?page={page}\"  # Construct page URL\n",
        "        print(f\"[Page {page}] ‚Üí {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        try:\n",
        "            # Wait for complaint elements to load on the page\n",
        "            WebDriverWait(driver, timeout).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"li.card.bpr-complaint-grid\"))\n",
        "            )\n",
        "        except:\n",
        "            print(f\"‚ö†Ô∏è No complaints found, skipping page {page}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(pause)  # Pause to ensure page is fully loaded\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")  # Parse HTML with BeautifulSoup\n",
        "        cards = soup.select(\"li.card.bpr-complaint-grid\")  # Select complaint cards\n",
        "        print(f\"  ‚Ä¢ Found {len(cards)} complaint cards\")\n",
        "\n",
        "        # 3) Loop through each complaint card and extract relevant fields\n",
        "        for card in cards:\n",
        "            date_filed = card.select_one(\"p.bpr-complaint-date span\")\n",
        "            complaint_type = card.select_one(\"div.bpr-complaint-type span\")\n",
        "            text_div = card.select_one(\"div.bpr-complaint-body > div\")\n",
        "            biz_date = card.select_one(\"p.bpr-complaint-business-response-date\")\n",
        "            biz_body = card.select_one(\"div.bpr-complaint-business-response-body\")\n",
        "            cust_date = card.select_one(\"p.bpr-customer-response-date\")\n",
        "            cust_body = card.select_one(\"div.bpr-customer-response-body\")\n",
        "            status = card.select_one(\"div.bpr-complaint-status-summary\")\n",
        "\n",
        "            # Append a dictionary of cleaned values to the output list\n",
        "            all_rows.append({\n",
        "                \"date_filed\":             date_filed.get_text(strip=True)            if date_filed else \"\",\n",
        "                \"complaint_type\":         complaint_type.get_text(strip=True)        if complaint_type else \"\",\n",
        "                \"complaint_text\":         text_div.get_text(\" \", strip=True)         if text_div else \"\",\n",
        "                \"business_response_date\": biz_date.get_text(strip=True)              if biz_date else \"\",\n",
        "                \"business_response\":      biz_body.get_text(\" \", strip=True)         if biz_body else \"\",\n",
        "                \"customer_response_date\": cust_date.get_text(strip=True)            if cust_date else \"\",\n",
        "                \"customer_response\":      cust_body.get_text(\" \", strip=True)        if cust_body else \"\",\n",
        "                \"status\":                 status.get_text(strip=True)               if status else \"\",\n",
        "                \"page\":                   page,  # Include source page number\n",
        "            })\n",
        "\n",
        "    driver.quit()  # Close browser when done\n",
        "    return pd.DataFrame(all_rows)  # Return results as a DataFrame"
      ],
      "metadata": {
        "id": "k23_rQ7KlMsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    BASE = \"https://www.bbb.org/us/ca/san-jose/profile/payment-processing-services/paypal-inc-1216-210387/complaints\"\n",
        "\n",
        "    # Run the scraper across 300 randomly selected pages (out of 2771)\n",
        "    df = scrape_bbb_complaints_random(BASE, min_page=1, max_page=2771, num_samples=300)\n",
        "\n",
        "    print(df.head())  # Print first few rows of the collected DataFrame\n",
        "\n",
        "    # File path for saving the scraped data\n",
        "    out_path = (\n",
        "        \"C:/Users/jthom/OneDrive/JOSH WORKING/SCHOOL WORK/\"\n",
        "        \"MS Business Analytics/GBA 6410 - Social Media Analytics and Text Mining/\"\n",
        "        \"Project/Paypal_bbb_complaints_random.csv\"\n",
        "    )\n",
        "\n",
        "    # Save the results as a CSV file\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"‚úÖ Saved {out_path}\")"
      ],
      "metadata": {
        "id": "0Y1wBBJilMmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code for Analysis**"
      ],
      "metadata": {
        "id": "KM1XVCpLlhbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG3Hrg6Laz7J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "\n",
        "from scipy.spatial.distance import pdist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "x37TYK8TbtM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "venmo_df = pd.read_csv(\"/content/Venmo_bbb_complaints_random.csv\")\n",
        "chime_df = pd.read_csv(\"/content/Chime_bbb_complaints_random.csv\")\n",
        "paypal_df = pd.read_csv(\"/content/Paypal_bbb_complaints_random.csv\")\n",
        "square_df = pd.read_csv(\"/content/Square_bbb_complaints_random.csv\")\n",
        "\n",
        "#Add company name to df\n",
        "venmo_df['Company'] = 'Venmo'\n",
        "chime_df['Company'] = 'Chime'\n",
        "paypal_df['Company'] = 'Paypal'\n",
        "square_df['Company'] = 'Square'\n",
        "\n",
        "#combine datasets from all companies\n",
        "df_complete = pd.concat([venmo_df, chime_df, paypal_df, square_df], ignore_index=True)\n",
        "df_complete.to_csv(\"/content/Complete_BBB_Corpus.csv\")"
      ],
      "metadata": {
        "id": "467PkWOEa8Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-Processing"
      ],
      "metadata": {
        "id": "1C8dgwtzwWgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text data preprocessing\n",
        "def preprocessing(data):\n",
        "\n",
        "    #convert all to string and convert to lowercase\n",
        "    data['complaint_text'] = data['complaint_text'].fillna('').astype(str).str.lower()\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean = []\n",
        "\n",
        "    for message in data['complaint_text']:\n",
        "\n",
        "        #remove punctuation\n",
        "        message = re.sub(r'[^\\w\\s]', '', message)\n",
        "\n",
        "        #tokenization\n",
        "        tokens = word_tokenize(message)\n",
        "\n",
        "        #Remove Stop words\n",
        "        filtered = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        #Remove lemma\n",
        "        filtered = [lemmatizer.lemmatize(word) for word in filtered]\n",
        "\n",
        "        clean.append(filtered)\n",
        "\n",
        "    return clean"
      ],
      "metadata": {
        "id": "pNrTwzQda-tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorization"
      ],
      "metadata": {
        "id": "QSBCwOXrwgaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorization(normalized_data):\n",
        "    # Join tokens into space-separated strings\n",
        "    joined_data = [' '.join(tokens) for tokens in normalized_data]\n",
        "\n",
        "    tv = TfidfVectorizer(min_df=0.10, max_df=0.95, norm='l2',\n",
        "                         use_idf=True, smooth_idf=True)\n",
        "\n",
        "    tv_matrix = tv.fit_transform(joined_data) # Fit the vectorizer on the text data and transform it into a TF-IDF matrix\n",
        "    tv_matrix = tv_matrix.toarray() # Convert the resulting sparse matrix to a dense NumPy array\n",
        "    vocab = tv.get_feature_names_out() # Extract the feature names (i.e., vocabulary terms)\n",
        "\n",
        "    # Create a DataFrame from the TF-IDF array for easier manipulation\n",
        "    vectorized = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
        "\n",
        "    return vectorized # Return the vectorized text data as a DataFrame\n",
        "\n",
        "normalized_data = preprocessing(df_complete)\n",
        "joined_texts = [' '.join(tokens) for tokens in normalized_data]\n",
        "\n",
        "# Call the function to create the TF-IDF feature matrix\n",
        "vectorized = vectorization(normalized_data)"
      ],
      "metadata": {
        "id": "7C6Trg7zbAzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the pairwise cosine similarity between all complaint vectors\n",
        "cosine_sim_matrix = pd.DataFrame(cosine_similarity(vectorized))\n",
        "\n",
        "# Extract the underlying NumPy array from the TF-IDF DataFrame\n",
        "tfidf_array = vectorized.values\n",
        "\n",
        "# Calculate the pairwise (condensed) Euclidean distances between all complaints\n",
        "condensed_euclidean_dist = pdist(tfidf_array, metric='euclidean')\n",
        "\n",
        "# Perform hierarchical clustering using Ward's linkage method\n",
        "Z = linkage(condensed_euclidean_dist, method='ward')"
      ],
      "metadata": {
        "id": "5Rev1kGfbCzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster and Vizualization"
      ],
      "metadata": {
        "id": "fsZCME70wkcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to Perform Clustering and Visualize with 2D PCA ---\n",
        "def cluster_and_visualize_2D(tfidf_array, Z, num_clusters, company_labels):\n",
        "\n",
        "    # Step 1: Assign cluster labels\n",
        "    cluster_labels = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
        "\n",
        "    # Step 2: PCA (2 Components)\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_data = pca.fit_transform(tfidf_array)\n",
        "\n",
        "    # Step 3: Build DataFrame for plotting\n",
        "    cluster_df = pd.DataFrame({\n",
        "        'PCA1': reduced_data[:, 0],\n",
        "        'PCA2': reduced_data[:, 1],\n",
        "        'Cluster': cluster_labels,\n",
        "        'Company': company_labels\n",
        "    })\n",
        "\n",
        "    # Step 4: 2D Plot ‚Äî color by cluster, shape by company\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(\n",
        "        data=cluster_df,\n",
        "        x='PCA1', y='PCA2',\n",
        "        hue='Cluster',\n",
        "        style='Company',\n",
        "        palette='tab10',\n",
        "        s=70,\n",
        "        alpha=0.8,\n",
        "        edgecolor='k'\n",
        "    )\n",
        "\n",
        "    plt.title(f'2D PCA Projection of Complaint Clusters (k={num_clusters})')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Cluster / Company\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return cluster_df\n",
        "\n",
        "# --- Call the function ---\n",
        "num_clusters = 4\n",
        "cluster_results_2d = cluster_and_visualize_2D(tfidf_array, Z, num_clusters, df_complete['Company'])"
      ],
      "metadata": {
        "id": "GR-hH-iym2Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized['Cluster'] = cluster_results_2d['Cluster'].values  # Add cluster labels to TF-IDF matrix"
      ],
      "metadata": {
        "id": "f21d1E-Tm07P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### n grams"
      ],
      "metadata": {
        "id": "CNgsec_MwoRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_ngrams_by_cluster(texts, cluster_labels, n=2, top_k=10):\n",
        "    df = pd.DataFrame({'text': texts, 'cluster': cluster_labels})\n",
        "    cluster_ngrams = {}\n",
        "\n",
        "    for cluster_id in sorted(df['cluster'].unique()):\n",
        "        print(f\"\\nüîç Top {n}-grams for Cluster {cluster_id}\")\n",
        "        cluster_texts = df[df['cluster'] == cluster_id]['text']\n",
        "\n",
        "        vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
        "        X = vectorizer.fit_transform(cluster_texts)\n",
        "        ngram_freq = X.sum(axis=0)\n",
        "\n",
        "        ngram_counts = [(ngram, int(ngram_freq[0, idx])) for ngram, idx in vectorizer.vocabulary_.items()]\n",
        "        top_ngrams = sorted(ngram_counts, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "        cluster_ngrams[cluster_id] = top_ngrams\n",
        "\n",
        "        for phrase, count in top_ngrams:\n",
        "            print(f\"{phrase}: {count}\")\n",
        "\n",
        "    return cluster_ngrams\n"
      ],
      "metadata": {
        "id": "EM43Ds1FAQqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ngrams_histograms_by_cluster(ngram_data):\n",
        "    clusters = sorted(ngram_data.keys())\n",
        "    num_clusters = len(clusters)\n",
        "\n",
        "    # Setup for a 2x2 grid\n",
        "    rows, cols = 2, 2\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(12, 10), constrained_layout=True)\n",
        "    axes = axes.flatten()  # make it easy to index\n",
        "\n",
        "    for i, cluster_id in enumerate(clusters):\n",
        "        top_ngrams = ngram_data[cluster_id]\n",
        "        phrases, counts = zip(*top_ngrams)\n",
        "\n",
        "        axes[i].barh(phrases[::-1], counts[::-1], color='skyblue')\n",
        "        axes[i].set_title(f\"Cluster {cluster_id}\")\n",
        "        axes[i].set_xlabel(\"Frequency\")\n",
        "        axes[i].set_ylabel(\"Top N-grams\")\n",
        "\n",
        "    # Hide any unused subplots (if < 4 clusters)\n",
        "    for j in range(len(clusters), len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.suptitle(\"Top N-grams by Cluster (2x2 Layout)\", fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MpayH6BRvpUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and return top n-grams\n",
        "ngram_data = top_ngrams_by_cluster(joined_texts, vectorized['Cluster'], n=2, top_k=10)\n",
        "\n",
        "# Plot using already extracted data\n",
        "plot_ngrams_histograms_by_cluster(ngram_data)"
      ],
      "metadata": {
        "id": "39zZcp0cwSRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for Severity_Score using OpenAI API (1hour to label 1000)"
      ],
      "metadata": {
        "id": "1amf4sFHSuPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import openai\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import csv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key from Colab Secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('JP_OPEN_AI_Key')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Check if API key is loaded\n",
        "if not api_key:\n",
        "    print(\"Error: No API key found. Please set JP_OPEN_AI_Key in Colab Secrets.\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize Open AI client\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Read the dataset with robust parsing\n",
        "try:\n",
        "    df = pd.read_csv(\"Complete_BBB_Corpus.csv\", quoting=csv.QUOTE_ALL, engine='python', on_bad_lines='skip')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Input file 'Complete_BBB_Corpus.csv' not found in the same directory.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error reading CSV: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Sample 1000 records or all if fewer than 1000\n",
        "sample_size = min(1000, len(df))\n",
        "try:\n",
        "    sampled_df = df.sample(n=sample_size, random_state=42)\n",
        "except ValueError as e:\n",
        "    print(f\"Error sampling data: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Function to generate sensitivity score using Open AI API\n",
        "def get_sensitivity_score(complaint_text):\n",
        "    if not isinstance(complaint_text, str):\n",
        "        return None, \"Error: Complaint text is missing or invalid.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a fraud and customer complaint analyst.\n",
        "    Evaluate the following customer complaint and assign a severity score between 0.0 and 1.0, reflecting the overall seriousness and business impact of the issue. This score should guide how urgently a company should respond or escalate the complaint.\n",
        "    Consider the following six factors:\n",
        "    Financial impact (e.g., significant losses, refunds, overcharges)\n",
        "    Emotional sensitivity (e.g., tone, distress, hardship expressed)\n",
        "    Fraud or security concerns (e.g., unauthorized transactions, identity theft)\n",
        "    Urgency (e.g., impact on essentials like rent, food, time-sensitive problems)\n",
        "    Customer service breakdown (e.g., repeated failures, no resolution offered)\n",
        "    Tone or language (e.g., legal threats, accusations, emotionally charged language)\n",
        "    Score guidelines:\n",
        "    0.0‚Äì0.2: Minor ‚Äî low impact, no urgency, easily resolvable\n",
        "    0.3‚Äì0.5: Moderate ‚Äî needs attention but not urgent or damaging\n",
        "    0.6‚Äì0.8: High ‚Äî serious issue, business should respond quickly\n",
        "    0.9‚Äì1.0: Critical ‚Äî urgent, high-risk to customer or company (legal, financial, PR)\n",
        "    Provide the severity score as a number and a brief rationale (1-3 sentences) explaining your reason for given that score.\n",
        "    Complaint text: \"{complaint_text}\"\n",
        "    Response format:\n",
        "    Score: <number>\n",
        "    Rationale: <explanation>\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a fraud and customer complaint analyst.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        result = response.choices[0].message.content.strip()\n",
        "        score_line = result.split(\"\\n\")[0]\n",
        "        rationale_line = result.split(\"\\n\")[1]\n",
        "        score = float(score_line.replace(\"Score: \", \"\"))\n",
        "        rationale = rationale_line.replace(\"Rationale: \", \"\")\n",
        "        return score, rationale\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing complaint: {e}\")\n",
        "        return None, f\"API error: {str(e)}\"\n",
        "\n",
        "# Process sampled complaints and collect results\n",
        "results = []\n",
        "for index, row in sampled_df.iterrows():\n",
        "    complaint_text = row['complaint_text']\n",
        "    row_id = index\n",
        "    complaint_id = row.get('complaint_id', str(uuid.uuid4()))\n",
        "\n",
        "    print(f\"Processing row_id: {row_id}, complaint_id: {complaint_id}\")\n",
        "    score, rationale = get_sensitivity_score(complaint_text)\n",
        "\n",
        "    results.append({\n",
        "        'row_id': row_id,\n",
        "        'complaint_id': complaint_id,\n",
        "        'sensitivity_score': score,\n",
        "        'rationale': rationale\n",
        "    })\n",
        "\n",
        "    time.sleep(1)  # Avoid rate limiting\n",
        "\n",
        "# Create DataFrame from results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save to CSV\n",
        "try:\n",
        "    results_df.to_csv(\"openai_sensitivity_scores.csv\", index=False)\n",
        "    print(f\"Output saved to 'openai_sensitivity_scores.csv' with {len(results_df)} records\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving output: {e}\")\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "hwzmhw0_6v2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "s5LeDvzwXjCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forrest"
      ],
      "metadata": {
        "id": "ajeJE3ruXwkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "# Load and merge files for training\n",
        "def load_and_merge_files(file1_path, file2_path, id_column='id', score_column='sensitivity_score'):\n",
        "    try:\n",
        "        df1 = pd.read_csv(file1_path, quoting=csv.QUOTE_ALL, engine='python', on_bad_lines='skip')\n",
        "        df2 = pd.read_csv(file2_path, quoting=csv.QUOTE_ALL, engine='python', on_bad_lines='skip')\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File not found - {e}\")\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    # Merge on the specified ID column\n",
        "    merged_df = pd.merge(df1, df2, on=id_column, how='inner')\n",
        "\n",
        "    # Map score to label\n",
        "    def map_score_to_label(score):\n",
        "        if 0.0 <= score <= 0.2:\n",
        "            return 'Minor'\n",
        "        elif 0.3 <= score <= 0.6:\n",
        "            return 'Moderate'\n",
        "        elif 0.7 <= score <= 0.8:\n",
        "            return 'High'\n",
        "        elif 0.9 <= score <= 1.0:\n",
        "            return 'Critical'\n",
        "        else:\n",
        "            return 'Invalid'\n",
        "\n",
        "    merged_df['sensitivity_label'] = merged_df[score_column].apply(map_score_to_label)\n",
        "    return merged_df, df1  # Return merged training data and full file_1\n",
        "\n",
        "# Train Random Forest model\n",
        "def train_random_forest_model(df, text_column='complaint_text', label_column='sensitivity_label'):\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "    df[text_column] = df[text_column].apply(clean_text)\n",
        "\n",
        "    # Remove labels with fewer than 2 samples\n",
        "    label_counts = df[label_column].value_counts()\n",
        "    valid_labels = label_counts[label_counts >= 2].index\n",
        "    df = df[df[label_column].isin(valid_labels)]\n",
        "\n",
        "    print(\"Filtered label counts:\\n\", df[label_column].value_counts())\n",
        "\n",
        "    # Stratified split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df[text_column], df[label_column],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df[label_column]\n",
        "    )\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train Random Forest\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    print(\"Classification Report on Training Data:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model, vectorizer\n",
        "\n",
        "# Predict labels on full dataset\n",
        "def predict_labels(full_df, model, vectorizer, text_column='complaint_text'):\n",
        "    if text_column not in full_df.columns:\n",
        "        print(f\"Error: Column '{text_column}' not found in the dataset.\")\n",
        "        return None\n",
        "\n",
        "    full_df['cleaned_text'] = full_df[text_column].apply(clean_text)\n",
        "    X_full = vectorizer.transform(full_df['cleaned_text'])\n",
        "    full_df['predicted_sensitivity_label'] = model.predict(X_full)\n",
        "    return full_df\n",
        "\n",
        "# File paths\n",
        "file_1 = \"Complete_BBB_Corpus.csv\"  # Full dataset\n",
        "file_2 = \"openai_sensitivity_scores2.csv\"  # Training data with sensitivity scores\n",
        "\n",
        "# Load and merge for training, get full file_1\n",
        "train_df, full_df = load_and_merge_files(file_1, file_2)\n",
        "\n",
        "# Train the model\n",
        "model, vectorizer = train_random_forest_model(train_df)\n",
        "\n",
        "# Save model and vectorizer\n",
        "joblib.dump(model, 'rf_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "print(\"Model and vectorizer saved as 'rf_model.pkl' and 'tfidf_vectorizer.pkl'\")\n",
        "\n",
        "# Predict on full file_1 dataset\n",
        "result_full_df = predict_labels(full_df, model, vectorizer)\n",
        "\n",
        "# Save or display results\n",
        "if result_full_df is not None:\n",
        "    output_path = \"predicted_full_complaints.csv\"\n",
        "    result_full_df.to_csv(output_path, index=False)\n",
        "    print(f\"Predictions saved to {output_path}\")\n",
        "    print(\"\\nSample of predictions:\")\n",
        "    print(result_full_df[['complaint_text', 'predicted_sensitivity_label']].head())\n",
        "else:\n",
        "    print(\"Failed to generate predictions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BIWsK7e-51u",
        "outputId": "de72863a-7242-4b14-bcce-e4720782f06a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered label counts:\n",
            " sensitivity_label\n",
            "High        490\n",
            "Critical    396\n",
            "Moderate    113\n",
            "Name: count, dtype: int64\n",
            "Classification Report on Training Data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Critical       0.68      0.57      0.62        79\n",
            "        High       0.59      0.80      0.68        98\n",
            "    Moderate       1.00      0.04      0.08        23\n",
            "\n",
            "    accuracy                           0.62       200\n",
            "   macro avg       0.76      0.47      0.46       200\n",
            "weighted avg       0.67      0.62      0.59       200\n",
            "\n",
            "Model and vectorizer saved as 'rf_model.pkl' and 'tfidf_vectorizer.pkl'\n",
            "Predictions saved to predicted_full_complaints.csv\n",
            "\n",
            "Sample of predictions:\n",
            "                                      complaint_text  \\\n",
            "0                                  Suspended account   \n",
            "1  Subject: Urgent: Fraudulent Activity on My Ven...   \n",
            "2  I contacted Venmo / Paypal customer support de...   \n",
            "3  I sold some test strips to Two Mom's Buy Test ...   \n",
            "4  I just found out my card was taken and has bee...   \n",
            "\n",
            "  predicted_sensitivity_label  \n",
            "0                        High  \n",
            "1                    Critical  \n",
            "2                    Critical  \n",
            "3                    Moderate  \n",
            "4                    Critical  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "FTnA38H5XqKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_logistic_regression_model(df, text_column='complaint_text', label_column='sensitivity_label'):\n",
        "    # Drop missing\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "\n",
        "    # Clean the text\n",
        "    df[text_column] = df[text_column].apply(clean_text)\n",
        "\n",
        "    # Remove labels with fewer than 2 samples\n",
        "    label_counts = df[label_column].value_counts()\n",
        "    valid_labels = label_counts[label_counts >= 2].index\n",
        "    df = df[df[label_column].isin(valid_labels)]\n",
        "\n",
        "    # Report final label distribution\n",
        "    print(\"Filtered label counts:\\n\", df[label_column].value_counts())\n",
        "\n",
        "    # Stratified split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df[text_column], df[label_column],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df[label_column]\n",
        "    )\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train Logistic Regression\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model, vectorizer\n",
        "\n",
        "\n",
        "logreg_model, logreg_vectorizer = train_logistic_regression_model(result_df)"
      ],
      "metadata": {
        "id": "rFMJIKrmeFIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e36a0c-c483-455a-bc34-c045d25a81d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered label counts:\n",
            " sensitivity_label\n",
            "High        490\n",
            "Critical    396\n",
            "Moderate    113\n",
            "Name: count, dtype: int64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Critical       0.59      0.52      0.55        79\n",
            "        High       0.55      0.73      0.63        98\n",
            "    Moderate       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.56       200\n",
            "   macro avg       0.38      0.42      0.39       200\n",
            "weighted avg       0.50      0.56      0.53       200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert"
      ],
      "metadata": {
        "id": "vOzDT16cXsYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def train_bert_model(df, text_column='complaint_text', label_column='sensitivity_label'):\n",
        "    # Drop missing values\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "\n",
        "    # Filter labels with at least 2 samples\n",
        "    df = df[df[label_column].map(df[label_column].value_counts()) >= 2]\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    df['label'] = le.fit_transform(df[label_column])\n",
        "\n",
        "    # Stratified split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_df, test_df = train_test_split(\n",
        "        df, test_size=0.2, random_state=42, stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Convert to Hugging Face Dataset\n",
        "    train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "    test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "    dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Tokenization function\n",
        "    def tokenize_function(example):\n",
        "        return tokenizer(\n",
        "            example[text_column],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        )\n",
        "\n",
        "    # Apply tokenization\n",
        "    dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Rename label column for Hugging Face Trainer\n",
        "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "    # Load model\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=len(le.classes_)\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    logging_dir=\"./logs\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test']\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    predictions = trainer.predict(dataset['test'])\n",
        "    y_pred = predictions.predictions.argmax(axis=1)\n",
        "    y_true = predictions.label_ids\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "\n",
        "    return model, tokenizer, le"
      ],
      "metadata": {
        "id": "TcOA3j7_euUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model, bert_tokenizer, label_encoder = train_bert_model(result_df)"
      ],
      "metadata": {
        "id": "fifPQ8-KeyJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Examples"
      ],
      "metadata": {
        "id": "KA2Bw2ivQx-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Critical Example - Complaint #48:\n",
        "\n",
        "on november 17th 2023 i was scammed using venmo  within a minute i realized i was scammed called my bank to stop payment and called venmo to stop payment   they all have record of my call   venmos policy states that this was a friend to friend exchange and they continued to front the 75000    i contacted venmo and followed their directions to file a police report which i did   since then venmo will not close my account as they say i have a negative balance   when someone accidently venmos me money it is just taken off the debt they say i owe   i am only asking venmo to close my account so this doesnt happen any more to my friends and family that think they can pay me by sending money via venmo    i did not lose any of my money as the bank stopped payment on 111723   however i will continue to lose money if they wont close my account"
      ],
      "metadata": {
        "id": "OpWLojNWQrtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low Example - Complaint #9\n",
        "i have been emailing venmo to unfreeze my account they said theyre going to hold my money for 180 days i just want to close that account and get my money back im really frustrated"
      ],
      "metadata": {
        "id": "mOlqj42FQw9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heat Map of Severity Score"
      ],
      "metadata": {
        "id": "KLfU3RavPren"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the merged_severity.csv file\n",
        "try:\n",
        "    df = pd.read_csv(\"merged_severity.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Input file 'merged_severity.csv' not found.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error reading CSV: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "\n",
        "# Define a function to create a pivot table with optional percentage calculation\n",
        "def create_pivot_table(df, index_col, column_col, value_col='id', as_percentage=False):\n",
        "    if index_col not in df.columns or column_col not in df.columns:\n",
        "        print(f\"Error: One or both of {index_col}, {column_col} not found in DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    # Create the pivot table with counts\n",
        "    pivot = df.groupby([index_col, column_col]).size().unstack(fill_value=0)\n",
        "\n",
        "    if as_percentage:\n",
        "        # Calculate percentages across the column_col\n",
        "        pivot = pivot.div(pivot.sum(axis=0), axis=1) * 100\n",
        "        pivot = pivot.round(2)  # Round to 2 decimal places\n",
        "    else:\n",
        "        print(f\"\\nCount of {index_col} per {column_col}:\")\n",
        "\n",
        "    return pivot\n",
        "\n",
        "# Define a function to calculate percentages from a pivoted DataFrame (for already pivoted data)\n",
        "def calculate_percentages(df, index_col='sensitivity_label'):\n",
        "    # Handle case where index is unnamed\n",
        "    index_name = df.index.name if df.index.name is not None else None\n",
        "    if index_name != index_col and index_col in df.columns:\n",
        "        df = df.set_index(index_col)\n",
        "    elif index_name != index_col and index_col not in df.columns:\n",
        "        print(f\"Error: {index_col} not found in DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    # Calculate percentages across each company\n",
        "    percentage_table = df.div(df.sum(axis=0), axis=1) * 100\n",
        "    percentage_table = percentage_table.round(2)  # Round to 2 decimal places\n",
        "\n",
        "    return percentage_table\n",
        "\n",
        "\n",
        "\n",
        "# Generate the percentage pivot table for sensitivity_label by Company\n",
        "pivot_table = create_pivot_table(df, index_col='sensitivity_label', column_col='Company', as_percentage=True)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "if pivot_table is not None:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(\n",
        "        pivot_table,\n",
        "        annot=True,  # Show percentage values in cells\n",
        "        fmt='.2f',   # Format numbers to 2 decimal places\n",
        "        cmap='YlOrRd',  # Color scheme (Yellow-Orange-Red)\n",
        "        cbar_kws={'label': 'Percentage (%)'}\n",
        "    )\n",
        "    plt.title('Percentage of Sensitivity Labels by Company')\n",
        "    plt.xlabel('Company')\n",
        "    plt.ylabel('Sensitivity Label')\n",
        "\n",
        "    # Save the heatmap to a file\n",
        "    # plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    # print(\"\\nHeatmap saved as 'heatmap.png'\")\n",
        "\n",
        "    # Display the plot (optional, depending on environment)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GQHfoAP9Pq1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Critical Complaints Analysis"
      ],
      "metadata": {
        "id": "IL2Y4NH7xK7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic for Identifying Department"
      ],
      "metadata": {
        "id": "9fzMlkibxW2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Import libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# 3. Download NLTK resources\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# 4. Load your Excel file\n",
        "df = pd.read_csv(\"predicted_full_complaints.csv\")\n",
        "\n",
        "# 5. Filter to only include 'Critical' complaints (sensitivity_score ‚â• 0.9)\n",
        "critical_df = df[df['predicted_sensitivity_label'] == 'Critical']#copy()\n",
        "\n",
        "# 6. Drop missing or empty complaints\n",
        "critical_df = critical_df.dropna(subset=['complaint_text'])\n",
        "critical_df = critical_df[critical_df['complaint_text'].str.strip() != \"\"]\n",
        "\n",
        "# 7. Define preprocessing function with keyword removal\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove target keywords\n",
        "    keywords_to_remove = [\"paypal\", \"chime\", \"venmo\", \"square\"]\n",
        "    for keyword in keywords_to_remove:\n",
        "        text = text.replace(keyword, \"\")\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "\n",
        "    # Tokenize\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Rejoin into string\n",
        "    return \" \".join(words)\n",
        "\n",
        "# 8. Apply preprocessing\n",
        "critical_df[\"cleaned_complaint\"] = critical_df[\"complaint_text\"].apply(preprocess_text)\n",
        "\n",
        "# 9. Convert text column to list\n",
        "complaints = critical_df[\"cleaned_complaint\"].tolist()\n",
        "\n",
        "# 10. Load sentence embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 11. Initialize BERTopic\n",
        "topic_model = BERTopic(embedding_model=embedding_model, verbose=True)\n",
        "\n",
        "# 12. Fit the model to the complaints\n",
        "topics, probs = topic_model.fit_transform(complaints)\n",
        "\n",
        "# Replace your model with the reduced version\n",
        "topic_model = topic_model.reduce_topics(complaints, nr_topics=10)\n",
        "\n",
        "\n",
        "# 13. Add results back to your DataFrame\n",
        "critical_df[\"topic\"] = topics\n",
        "critical_df[\"probability\"] = probs\n",
        "\n",
        "# 14. Save clustered results (optional)\n",
        "critical_df.to_excel(\"critical_complaints_clustered.xlsx\", index=False)\n",
        "\n",
        "# 15. View top topics\n",
        "print(topic_model.get_topic_info().head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621,
          "referenced_widgets": [
            "f84c60031af349dc8a4b0abaf99b6343",
            "d7b85a9dc24c4a0f9567723f27614f7a",
            "0058dd4a235546fc9a9d54fc3f09325c",
            "0b4795ab34944dd5b8ad529c843ba34a",
            "30a790cf7f214e2e97eb9fdd7f46c566",
            "7a2adc15cf7a4e3ab8adf48dc5fd0805",
            "e1b9990b5b4b4992a96c1928264498f3",
            "9ec27afaf780493fb7eaca432590f55e",
            "8689874f71e3435794e450053a251b2a",
            "0e159ca3f79645c5870a6a1c0e4f66e2",
            "57321e4bd029481bb9437d8a3d89cf33"
          ]
        },
        "id": "urCUkM0lIz07",
        "outputId": "4d597300-35d3-4fcc-cf60-54736714cbcb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-19 19:24:52,740 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/113 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f84c60031af349dc8a4b0abaf99b6343"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-19 19:28:32,166 - BERTopic - Embedding - Completed ‚úì\n",
            "2025-07-19 19:28:32,167 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2025-07-19 19:28:55,085 - BERTopic - Dimensionality - Completed ‚úì\n",
            "2025-07-19 19:28:55,089 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2025-07-19 19:28:55,477 - BERTopic - Cluster - Completed ‚úì\n",
            "2025-07-19 19:28:55,500 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
            "2025-07-19 19:28:55,967 - BERTopic - Representation - Completed ‚úì\n",
            "2025-07-19 19:28:56,354 - BERTopic - Topic reduction - Reducing number of topics\n",
            "2025-07-19 19:28:56,383 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
            "2025-07-19 19:28:56,833 - BERTopic - Representation - Completed ‚úì\n",
            "2025-07-19 19:28:56,841 - BERTopic - Topic reduction - Reduced number of topics from 38 to 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Topic  Count                                 Name  \\\n",
            "0     -1   1764    -1_account_money_transaction_card   \n",
            "1      0   1178           0_account_money_email_fund   \n",
            "2      1    407  1_dispute_charge_transaction_refund   \n",
            "3      2     93             2_cash_app_money_account   \n",
            "4      3     74         3_phone_money_stolen_dispute   \n",
            "\n",
            "                                      Representation  \\\n",
            "0  [account, money, transaction, card, dispute, b...   \n",
            "1  [account, money, email, fund, business, day, i...   \n",
            "2  [dispute, charge, transaction, refund, claim, ...   \n",
            "3  [cash, app, money, account, back, transaction,...   \n",
            "4  [phone, money, stolen, dispute, incarcerated, ...   \n",
            "\n",
            "                                 Representative_Docs  \n",
            "0  [use business client use credit card june th a...  \n",
            "1  [october decided close account fine prior clos...  \n",
            "2  [refused refund unauthorized charge denied cla...  \n",
            "3  [charge cash app card fraudulent scam cash app...  \n",
            "4  [visiting family phone card stolen got new pho...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define topic label mapping\n",
        "topic_labels = {\n",
        "    -1: \"General Financial Transactions\",\n",
        "     0: \"Account Activity & Notifications\",\n",
        "     1: \"Transaction Disputes & Refunds\",\n",
        "     2: \"Mobile App & Cash Management\",\n",
        "     3: \"Fraud, Theft & Incarcerated Individuals\",\n",
        "     4: \"Amazon Account & Prime Issues\",\n",
        "     5: \"Rental & Lease Payment Issues\",\n",
        "     6: \"Credit Reporting & Experian\",\n",
        "     7: \"Deceased Accounts & Documentation\",\n",
        "     8: \"Account Deactivation & Escalation Requests\"\n",
        "}\n",
        "\n",
        "topics_in_model = topic_model.get_topics().keys()\n",
        "ordered_labels = [topic_labels.get(topic, f\"Topic {topic}\") for topic in topics_in_model]\n",
        "\n",
        "print(ordered_labels)\n",
        "\n",
        "# Set new labels\n",
        "topic_model.set_topic_labels(ordered_labels)\n",
        "\n",
        "# 2. Apply the mapping to your BERTopic-labeled DataFrame\n",
        "critical_df[\"topic_label\"] = critical_df[\"topic\"].map(topic_labels)\n",
        "\n",
        "# 3. Preview the mapping (optional)\n",
        "print(critical_df[[\"topic\", \"topic_label\"]].drop_duplicates().sort_values(\"topic\").head(10))\n",
        "\n",
        "# 4. (Optional) Save updated file\n",
        "critical_df.to_excel(\"critical_complaints_labeled.xlsx\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49XpPRZzI12K",
        "outputId": "2019c76f-bcc9-499f-9efb-2396f8c649ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['General Financial Transactions', 'Account Activity & Notifications', 'Transaction Disputes & Refunds', 'Mobile App & Cash Management', 'Fraud, Theft & Incarcerated Individuals', 'Amazon Account & Prime Issues', 'Rental & Lease Payment Issues', 'Credit Reporting & Experian', 'Deceased Accounts & Documentation', 'Account Deactivation & Escalation Requests']\n",
            "     topic                                 topic_label\n",
            "1       -1              General Financial Transactions\n",
            "47       0            Account Activity & Notifications\n",
            "27       1              Transaction Disputes & Refunds\n",
            "210      2                Mobile App & Cash Management\n",
            "269      3     Fraud, Theft & Incarcerated Individuals\n",
            "6        4               Amazon Account & Prime Issues\n",
            "28       5               Rental & Lease Payment Issues\n",
            "78       6                 Credit Reporting & Experian\n",
            "38       7           Deceased Accounts & Documentation\n",
            "104      8  Account Deactivation & Escalation Requests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_num = 1\n",
        "label = topic_model.topic_labels_[topic_num]\n",
        "print(f\"Label for topic {topic_num}: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss9nemM1I3br",
        "outputId": "6f6cc511-5bbb-4f66-b4c1-bcc9682d81a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label for topic 1: 1_dispute_charge_transaction_refund\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# 14. Save clustered results (optional)\n",
        "critical_df.to_excel(\"critical_complaints_clustered.xlsx\", index=False)\n",
        "\n",
        "# 15. View top topics\n",
        "print(topic_model.get_topic_info().head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwcA4v9kI5WU",
        "outputId": "80970dd7-d7ab-4768-c8d8-b69cc2eb624f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Topic  Count                                     Name  \\\n",
            "0     -1   1764        -1_account_money_transaction_card   \n",
            "1      0   1178               0_account_money_email_fund   \n",
            "2      1    407      1_dispute_charge_transaction_refund   \n",
            "3      2     93                 2_cash_app_money_account   \n",
            "4      3     74             3_phone_money_stolen_dispute   \n",
            "5      4     19               4_amazon_card_refund_prime   \n",
            "6      5     18           5_rent_landlord_money_property   \n",
            "7      6     16  6_transaction_unauthorized_made_dispute   \n",
            "8      7     16        7_mypay_payment_repayment_feature   \n",
            "9      8     11       8_credit_report_experian_reporting   \n",
            "\n",
            "                                   CustomName  \\\n",
            "0              General Financial Transactions   \n",
            "1            Account Activity & Notifications   \n",
            "2              Transaction Disputes & Refunds   \n",
            "3                Mobile App & Cash Management   \n",
            "4     Fraud, Theft & Incarcerated Individuals   \n",
            "5               Amazon Account & Prime Issues   \n",
            "6               Rental & Lease Payment Issues   \n",
            "7                 Credit Reporting & Experian   \n",
            "8           Deceased Accounts & Documentation   \n",
            "9  Account Deactivation & Escalation Requests   \n",
            "\n",
            "                                      Representation  \\\n",
            "0  [account, money, transaction, card, dispute, b...   \n",
            "1  [account, money, email, fund, business, day, i...   \n",
            "2  [dispute, charge, transaction, refund, claim, ...   \n",
            "3  [cash, app, money, account, back, transaction,...   \n",
            "4  [phone, money, stolen, dispute, incarcerated, ...   \n",
            "5  [amazon, card, refund, prime, account, dispute...   \n",
            "6  [rent, landlord, money, property, pay, lease, ...   \n",
            "7  [transaction, unauthorized, made, dispute, den...   \n",
            "8  [mypay, payment, repayment, feature, pay, mone...   \n",
            "9  [credit, report, experian, reporting, balance,...   \n",
            "\n",
            "                                 Representative_Docs  \n",
            "0  [use business client use credit card june th a...  \n",
            "1  [october decided close account fine prior clos...  \n",
            "2  [refused refund unauthorized charge denied cla...  \n",
            "3  [charge cash app card fraudulent scam cash app...  \n",
            "4  [visiting family phone card stolen got new pho...  \n",
            "5  [alerted fraudulent transaction account text m...  \n",
            "6  [july th transaction made account pay deposit ...  \n",
            "7  [february pm purchase made using account knowi...  \n",
            "8  [signed recently take online person payment re...  \n",
            "9  [updating credit file since december show repo...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = {\n",
        "    -1: \"General Financial Transactions\",\n",
        "     0: \"Account Activity & Notifications\",\n",
        "     1: \"Transaction Disputes & Refunds\",\n",
        "     2: \"Mobile App & Cash Management\",\n",
        "     3: \"Fraud, Theft & Incarcerated Individuals\",\n",
        "     4: \"Amazon Account & Prime Issues\",\n",
        "     5: \"Rental & Lease Payment Issues\",\n",
        "     6: \"Credit Reporting & Experian\",\n",
        "     7: \"Deceased Accounts & Documentation\",\n",
        "     8: \"Account Deactivation & Escalation Requests\"\n",
        "}\n",
        "\n",
        "# Reformat labels to include <br> for line breaks\n",
        "wrapped_labels = {\n",
        "    key: val.replace(\" & \", \"<br>& \") for key, val in topic_labels.items()\n",
        "}\n",
        "# Set custom labels to BERTopic model\n",
        "ordered_labels = [wrapped_labels.get(topic, f\"Topic {topic}\") for topic in topic_model.get_topics().keys()]\n",
        "topic_model.set_topic_labels(ordered_labels)\n",
        "\n",
        "# Generate bar chart with wrapped labels\n",
        "fig = topic_model.visualize_barchart(top_n_topics=10, custom_labels=True,title='<b>Critical Complaint Word Scores</b><br>')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "RzicgU-II6VA",
        "outputId": "92fb4a1e-2a57-4465-ecf9-6fac32464dea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1838bfce-6690-4fd7-aa3a-72971d800750\" class=\"plotly-graph-div\" style=\"height:750px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1838bfce-6690-4fd7-aa3a-72971d800750\")) {                    Plotly.newPlot(                        \"1838bfce-6690-4fd7-aa3a-72971d800750\",                        [{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.0256870775946902,0.026032012429080785,0.03182532911286753,0.03268566925538855,0.060952435626399064],\"y\":[\"business  \",\"fund  \",\"email  \",\"money  \",\"account  \"],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"#0072B2\"},\"orientation\":\"h\",\"x\":[0.025926688042452926,0.027092912088499108,0.027699728345892682,0.03502093992536713,0.04414448152728632],\"y\":[\"claim  \",\"refund  \",\"transaction  \",\"charge  \",\"dispute  \"],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"#CC79A7\"},\"orientation\":\"h\",\"x\":[0.028587681199816535,0.046505320986118696,0.04966091483000576,0.1618337701149143,0.16246281083080494],\"y\":[\"back  \",\"account  \",\"money  \",\"app  \",\"cash  \"],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"#E69F00\"},\"orientation\":\"h\",\"x\":[0.04169401309360284,0.04177617963774893,0.04873185334532959,0.049402724022727246,0.0629109472577549],\"y\":[\"incarcerated  \",\"dispute  \",\"stolen  \",\"money  \",\"phone  \"],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":\"#56B4E9\"},\"orientation\":\"h\",\"x\":[0.03847544399825369,0.040931756401785015,0.04358676036498169,0.05933645972771813,0.2112898285142477],\"y\":[\"account  \",\"prime  \",\"refund  \",\"card  \",\"amazon  \"],\"type\":\"bar\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"marker\":{\"color\":\"#009E73\"},\"orientation\":\"h\",\"x\":[0.03480856359702879,0.03816611930953593,0.04449569258055621,0.049608111414980574,0.08997263954177717],\"y\":[\"pay  \",\"property  \",\"money  \",\"landlord  \",\"rent  \"],\"type\":\"bar\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"marker\":{\"color\":\"#F0E442\"},\"orientation\":\"h\",\"x\":[0.07542730843704068,0.0810260680646089,0.08721450871572525,0.1341120863768912,0.20666261629006996],\"y\":[\"denied  \",\"dispute  \",\"made  \",\"unauthorized  \",\"transaction  \"],\"type\":\"bar\",\"xaxis\":\"x7\",\"yaxis\":\"y7\"},{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.04356225455520966,0.04578963306284097,0.05230545025139233,0.06120501157950083,0.06852079852561928],\"y\":[\"pay  \",\"feature  \",\"repayment  \",\"payment  \",\"mypay  \"],\"type\":\"bar\",\"xaxis\":\"x8\",\"yaxis\":\"y8\"},{\"marker\":{\"color\":\"#0072B2\"},\"orientation\":\"h\",\"x\":[0.07321112441916207,0.07597414095149024,0.0921600321677735,0.1041452545545173,0.19962996515987416],\"y\":[\"balance  \",\"reporting  \",\"experian  \",\"report  \",\"credit  \"],\"type\":\"bar\",\"xaxis\":\"x9\",\"yaxis\":\"y9\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7555555555555555,1.0],\"showgrid\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7555555555555555,1.0],\"showgrid\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.7555555555555555,1.0],\"showgrid\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.7555555555555555,1.0],\"showgrid\":true},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.37777777777777777,0.6222222222222222],\"showgrid\":true},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.37777777777777777,0.6222222222222222],\"showgrid\":true},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.37777777777777777,0.6222222222222222],\"showgrid\":true},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.37777777777777777,0.6222222222222222],\"showgrid\":true},\"xaxis9\":{\"anchor\":\"y9\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis9\":{\"anchor\":\"x9\",\"domain\":[0.0,0.24444444444444446],\"showgrid\":true},\"xaxis10\":{\"anchor\":\"y10\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis10\":{\"anchor\":\"x10\",\"domain\":[0.0,0.24444444444444446],\"showgrid\":true},\"xaxis11\":{\"anchor\":\"y11\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis11\":{\"anchor\":\"x11\",\"domain\":[0.0,0.24444444444444446],\"showgrid\":true},\"xaxis12\":{\"anchor\":\"y12\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis12\":{\"anchor\":\"x12\",\"domain\":[0.0,0.24444444444444446],\"showgrid\":true},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Account Activity\\u003cbr\\u003e& Notifications\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Transaction Disputes\\u003cbr\\u003e& Refunds\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Mobile App\\u003cbr\\u003e& Cash Management\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Fraud, Theft\\u003cbr\\u003e& Incarcerated Individuals\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Amazon Account\\u003cbr\\u003e& Prime Issues\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6222222222222222,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Rental\\u003cbr\\u003e& Lease Payment Issues\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6222222222222222,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Credit Reporting\\u003cbr\\u003e& Experian\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6222222222222222,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Deceased Accounts\\u003cbr\\u003e& Documentation\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6222222222222222,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Account Deactivation\\u003cbr\\u003e& Escalation Requests\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.24444444444444446,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eCritical Complaint Word Scores\\u003c\\u002fb\\u003e\\u003cbr\\u003e\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"showlegend\":false,\"width\":1000,\"height\":750},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1838bfce-6690-4fd7-aa3a-72971d800750');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Visualize topics (in notebook)\n",
        "topic_model.visualize_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "tcbu_YUzJALh",
        "outputId": "feba3445-f3ce-450f-e461-ad37e7fe3b56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"204a2db0-5d0e-4bda-8a62-845cff8fedf5\" class=\"plotly-graph-div\" style=\"height:650px; width:650px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"204a2db0-5d0e-4bda-8a62-845cff8fedf5\")) {                    Plotly.newPlot(                        \"204a2db0-5d0e-4bda-8a62-845cff8fedf5\",                        [{\"customdata\":[[0,\"account | money | email | fund | business\",1178],[1,\"dispute | charge | transaction | refund | claim\",407],[2,\"cash | app | money | account | back\",93],[3,\"phone | money | stolen | dispute | incarcerated\",74],[4,\"amazon | card | refund | prime | account\",19],[5,\"rent | landlord | money | property | pay\",18],[6,\"transaction | unauthorized | made | dispute | denied\",16],[7,\"mypay | payment | repayment | feature | pay\",16],[8,\"credit | report | experian | reporting | balance\",11]],\"hovertemplate\":\"\\u003cb\\u003eTopic %{customdata[0]}\\u003c\\u002fb\\u003e\\u003cbr\\u003e%{customdata[1]}\\u003cbr\\u003eSize: %{customdata[2]}\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#B0BEC5\",\"size\":[1178,407,93,74,19,18,16,16,11],\"sizemode\":\"area\",\"sizeref\":0.73625,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":2}},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[7.7174807,6.6620297,8.110659,7.5096407,5.7461023,7.3098416,6.11046,8.407765,6.694746],\"xaxis\":\"x\",\"y\":[2.554473,4.7840815,2.1609273,0.9586283,3.868309,1.9653817,4.82111,1.2619201,4.2757583],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"visible\":false,\"range\":[4.8841869831085205,9.668930196762085]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"visible\":false,\"range\":[0.8148340523242951,5.544276237487793]},\"legend\":{\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eIntertopic Distance Map\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":650,\"height\":650,\"sliders\":[{\"active\":0,\"pad\":{\"t\":50},\"steps\":[{\"args\":[{\"marker.color\":[[\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 0\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 1\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 2\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 3\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 4\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 5\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 6\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\"]]}],\"label\":\"Topic 7\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\"]]}],\"label\":\"Topic 8\",\"method\":\"update\"}]}],\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":7.276558589935303,\"x1\":7.276558589935303,\"y0\":0.8148340523242951,\"y1\":5.544276237487793},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":4.8841869831085205,\"x1\":9.668930196762085,\"y0\":3.179555144906044,\"y1\":3.179555144906044}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":4.8841869831085205,\"y\":3.179555144906044,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":7.276558589935303,\"xshift\":10,\"y\":5.544276237487793}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('204a2db0-5d0e-4bda-8a62-845cff8fedf5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See top keywords for each topic\n",
        "for topic_id in critical_df['topic'].unique():\n",
        "    print(f\"\\nTopic {topic_id}:\")\n",
        "    print(topic_model.get_topic(topic_id))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwZDDXXTJBn9",
        "outputId": "cb76f27e-3888-41b3-b40b-be29a170526c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic -1:\n",
            "[('account', np.float64(0.04389187972811434)), ('money', np.float64(0.03776178314599753)), ('transaction', np.float64(0.027348342511318055)), ('card', np.float64(0.025102658483435722)), ('dispute', np.float64(0.023509063157252857)), ('back', np.float64(0.022954419241522885)), ('bank', np.float64(0.020366847822012438)), ('time', np.float64(0.020155583963155966)), ('email', np.float64(0.019974702667002294)), ('would', np.float64(0.01981349126756171))]\n",
            "\n",
            "Topic 4:\n",
            "[('amazon', np.float64(0.2112898285142477)), ('card', np.float64(0.05933645972771813)), ('refund', np.float64(0.04358676036498169)), ('prime', np.float64(0.040931756401785015)), ('account', np.float64(0.03847544399825369)), ('dispute', np.float64(0.03676608991252376)), ('money', np.float64(0.03568194983808442)), ('charge', np.float64(0.03248040014902991)), ('bank', np.float64(0.03246411991775991)), ('email', np.float64(0.03084085281771949))]\n",
            "\n",
            "Topic 1:\n",
            "[('dispute', np.float64(0.04414448152728632)), ('charge', np.float64(0.03502093992536713)), ('transaction', np.float64(0.027699728345892682)), ('refund', np.float64(0.027092912088499108)), ('claim', np.float64(0.025926688042452926)), ('card', np.float64(0.024178315434178446)), ('denied', np.float64(0.023985061738005062)), ('filed', np.float64(0.02190328240068815)), ('case', np.float64(0.021355478671455947)), ('fraudulent', np.float64(0.021094215552786948))]\n",
            "\n",
            "Topic 5:\n",
            "[('rent', np.float64(0.08997263954177717)), ('landlord', np.float64(0.049608111414980574)), ('money', np.float64(0.04449569258055621)), ('property', np.float64(0.03816611930953593)), ('pay', np.float64(0.03480856359702879)), ('lease', np.float64(0.03387218664049819)), ('back', np.float64(0.03308511418300502)), ('month', np.float64(0.03273270178070019)), ('time', np.float64(0.03272698539020647)), ('payment', np.float64(0.032694611427847395))]\n",
            "\n",
            "Topic 7:\n",
            "[('mypay', np.float64(0.06852079852561928)), ('payment', np.float64(0.06120501157950083)), ('repayment', np.float64(0.05230545025139233)), ('feature', np.float64(0.04578963306284097)), ('pay', np.float64(0.04356225455520966)), ('money', np.float64(0.041682722452520415)), ('paid', np.float64(0.037313833449265296)), ('advance', np.float64(0.03479984907753898)), ('called', np.float64(0.030937816813417507)), ('customer', np.float64(0.03009400777363547))]\n",
            "\n",
            "Topic 0:\n",
            "[('account', np.float64(0.060952435626399064)), ('money', np.float64(0.03268566925538855)), ('email', np.float64(0.03182532911286753)), ('fund', np.float64(0.026032012429080785)), ('business', np.float64(0.0256870775946902)), ('day', np.float64(0.023227627430159763)), ('information', np.float64(0.020573660060993256)), ('would', np.float64(0.020553854288185458)), ('get', np.float64(0.019543122367266397)), ('told', np.float64(0.019428336498706))]\n",
            "\n",
            "Topic 31:\n",
            "False\n",
            "\n",
            "Topic 20:\n",
            "False\n",
            "\n",
            "Topic 12:\n",
            "False\n",
            "\n",
            "Topic 6:\n",
            "[('transaction', np.float64(0.20666261629006996)), ('unauthorized', np.float64(0.1341120863768912)), ('made', np.float64(0.08721450871572525)), ('dispute', np.float64(0.0810260680646089)), ('denied', np.float64(0.07542730843704068)), ('account', np.float64(0.06969961532106012)), ('authorize', np.float64(0.06445399946474448)), ('charged', np.float64(0.048168830541472966)), ('approved', np.float64(0.04548385652667071)), ('pm', np.float64(0.04199918113256799))]\n",
            "\n",
            "Topic 8:\n",
            "[('credit', np.float64(0.19962996515987416)), ('report', np.float64(0.1041452545545173)), ('experian', np.float64(0.0921600321677735)), ('reporting', np.float64(0.07597414095149024)), ('balance', np.float64(0.07321112441916207)), ('builder', np.float64(0.06600003535103638)), ('usage', np.float64(0.05815455684027827)), ('bureau', np.float64(0.057675439554702834)), ('show', np.float64(0.04697584576517859)), ('score', np.float64(0.04511607357480891))]\n",
            "\n",
            "Topic 13:\n",
            "False\n",
            "\n",
            "Topic 9:\n",
            "False\n",
            "\n",
            "Topic 32:\n",
            "False\n",
            "\n",
            "Topic 2:\n",
            "[('cash', np.float64(0.16246281083080494)), ('app', np.float64(0.1618337701149143)), ('money', np.float64(0.04966091483000576)), ('account', np.float64(0.046505320986118696)), ('back', np.float64(0.028587681199816535)), ('transaction', np.float64(0.025660446630349235)), ('get', np.float64(0.025606612089287936)), ('card', np.float64(0.022974944658814812)), ('sent', np.float64(0.02285073378763009)), ('told', np.float64(0.020629460980921898))]\n",
            "\n",
            "Topic 11:\n",
            "False\n",
            "\n",
            "Topic 3:\n",
            "[('phone', np.float64(0.0629109472577549)), ('money', np.float64(0.049402724022727246)), ('stolen', np.float64(0.04873185334532959)), ('dispute', np.float64(0.04177617963774893)), ('incarcerated', np.float64(0.04169401309360284)), ('account', np.float64(0.036963134710269366)), ('stole', np.float64(0.03684504905001011)), ('jail', np.float64(0.034110228890249186)), ('transaction', np.float64(0.032991060582837864)), ('filed', np.float64(0.030790102543068174))]\n",
            "\n",
            "Topic 10:\n",
            "False\n",
            "\n",
            "Topic 29:\n",
            "False\n",
            "\n",
            "Topic 30:\n",
            "False\n",
            "\n",
            "Topic 26:\n",
            "False\n",
            "\n",
            "Topic 33:\n",
            "False\n",
            "\n",
            "Topic 19:\n",
            "False\n",
            "\n",
            "Topic 23:\n",
            "False\n",
            "\n",
            "Topic 24:\n",
            "False\n",
            "\n",
            "Topic 14:\n",
            "False\n",
            "\n",
            "Topic 28:\n",
            "False\n",
            "\n",
            "Topic 17:\n",
            "False\n",
            "\n",
            "Topic 22:\n",
            "False\n",
            "\n",
            "Topic 25:\n",
            "False\n",
            "\n",
            "Topic 21:\n",
            "False\n",
            "\n",
            "Topic 34:\n",
            "False\n",
            "\n",
            "Topic 18:\n",
            "False\n",
            "\n",
            "Topic 27:\n",
            "False\n",
            "\n",
            "Topic 15:\n",
            "False\n",
            "\n",
            "Topic 35:\n",
            "False\n",
            "\n",
            "Topic 16:\n",
            "False\n",
            "\n",
            "Topic 36:\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_counts = critical_df[\"topic_label\"].value_counts()\n",
        "print(\"\\nComplaint counts by topic:\")\n",
        "print(topic_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I41XRq3MJDtB",
        "outputId": "98fedd93-e5fc-4771-cda3-c5708756ade9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Complaint counts by topic:\n",
            "topic_label\n",
            "General Financial Transactions                1764\n",
            "Account Activity & Notifications               330\n",
            "Transaction Disputes & Refunds                 190\n",
            "Mobile App & Cash Management                   176\n",
            "Fraud, Theft & Incarcerated Individuals         93\n",
            "Amazon Account & Prime Issues                   81\n",
            "Rental & Lease Payment Issues                   80\n",
            "Credit Reporting & Experian                     79\n",
            "Deceased Accounts & Documentation               76\n",
            "Account Deactivation & Escalation Requests      72\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - Agent Model"
      ],
      "metadata": {
        "id": "bdmAK0u2EyWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openai\n",
        "import faiss # Vector database library\n",
        "import os\n",
        "\n",
        "# --- 1. Setup and Configuration ---\n",
        "# It's best practice to set your API key as an environment variable\n",
        "# You can get an API key from https://platform.openai.com/\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# For this example, I'll assume the key is set. If not, uncomment the line above.\n",
        "\n",
        "# --- 2. Load Your Analyzed Data ---\n",
        "# [cite_start]We'll use the labeled critical complaints file you created[cite: 205].\n",
        "try:\n",
        "    df = pd.read_excel(\"critical_complaints_labeled.xlsx\")\n",
        "    print(\"Successfully loaded the complaint data.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'critical_complaints_labeled.xlsx' not found. Make sure the file is in the same directory.\")\n",
        "    exit()\n",
        "\n",
        "# Let's assume your excel file has columns like 'complaint_text', 'company', 'severity_score', 'topic_label'\n",
        "# We will create a single text field for embedding to give the model rich context.\n",
        "df['embedding_text'] = \"Company: \" + df['company'] + \"; Topic: \" + df['topic_label'] + \"; Complaint: \" + df['complaint_text']\n",
        "\n",
        "# --- 3. Generate Embeddings for the Complaints (The \"Indexing\" Step) ---\n",
        "# This only needs to be done once for your dataset.\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "embeddings = []\n",
        "\n",
        "# Check if embeddings are already saved to avoid re-running this expensive step\n",
        "if os.path.exists(\"complaint_embeddings.npy\"):\n",
        "    print(\"Loading existing embeddings from file.\")\n",
        "    embeddings = np.load(\"complaint_embeddings.npy\")\n",
        "else:\n",
        "    print(\"Generating new embeddings for all complaints. This may take a while...\")\n",
        "    # It's better to process in batches for a real application\n",
        "    for text in df['embedding_text']:\n",
        "        response = openai.embeddings.create(input=text, model=embedding_model)\n",
        "        embeddings.append(response.data[0].embedding)\n",
        "    embeddings = np.array(embeddings)\n",
        "    np.save(\"complaint_embeddings.npy\", embeddings) # Save for future use\n",
        "    print(\"Embeddings generated and saved.\")\n",
        "\n",
        "# --- 4. Build the Vector Database (The Retriever) ---\n",
        "# The dimension of the embeddings depends on the model used. 'text-embedding-3-small' has 1536 dimensions.\n",
        "d = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d) # Using a simple L2 distance index\n",
        "index.add(embeddings.astype('float32')) # Add the complaint vectors to the index\n",
        "\n",
        "print(f\"Retriever built successfully with {index.ntotal} complaint vectors.\")\n",
        "\n",
        "# --- 5. The RAG Question-Answering Function ---\n",
        "def answer_business_question(question: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Retrieves relevant complaints and uses an LLM to generate a nuanced answer.\n",
        "    \"\"\"\n",
        "    print(f\"\\nReceived question: '{question}'\")\n",
        "\n",
        "    # Step 1: Embed the user's question\n",
        "    print(\"--> Embedding the question...\")\n",
        "    question_embedding = openai.embeddings.create(input=question, model=embedding_model).data[0].embedding\n",
        "    question_embedding = np.array([question_embedding]).astype('float32')\n",
        "\n",
        "    # Step 2: Retrieve the most relevant complaints from the vector database\n",
        "    print(f\"--> Retrieving top {top_k} relevant complaints...\")\n",
        "    distances, indices = index.search(question_embedding, top_k)\n",
        "    retrieved_complaints = df.iloc[indices[0]]\n",
        "\n",
        "    # Step 3: Build the context for the generator LLM\n",
        "    context = \"You are an expert FinTech analyst. Your task is to answer the user's question based *only* on the context provided below from customer complaints.\\n\\n\"\n",
        "    context += \"--- CONTEXT ---\\n\"\n",
        "    for i, row in retrieved_complaints.iterrows():\n",
        "        context += f\"Complaint {i+1} (Company: {row['company']}, Topic: {row['topic_label']}): {row['complaint_text']}\\n\\n\"\n",
        "    context += \"--- QUESTION ---\\n\"\n",
        "    context += f\"{question}\\n\\n\"\n",
        "    context += \"--- ANSWER ---\\n\"\n",
        "    print(\"--> Generating answer with LLM...\")\n",
        "\n",
        "    # Step 4: Call the generator LLM to get the final answer\n",
        "    response = openai.chat.completions.create(\n",
        "        [cite_start]model=\"gpt-4o-mini\", # Using the same model for consistency [cite: 150, 238]\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": context}\n",
        "        ],\n",
        "        temperature=0.2 # Lower temperature for more factual, less creative answers\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# --- 6. Example Usage ---\n",
        "# Now, let's ask the question from the previous explanation.\n",
        "business_question = \"What are the common triggers for sudden deactivation of Square business accounts, and what is the typical financial impact on users?\"\n",
        "final_answer = answer_business_question(business_question)\n",
        "\n",
        "print(\"\\n==================== FINAL ANSWER ====================\")\n",
        "print(final_answer)\n",
        "print(\"======================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "zxCK3GrEE05c",
        "outputId": "614e09f0-924c-42c7-e99f-72c2d3589fcd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-19-3747906042.py, line 83)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-19-3747906042.py\"\u001b[0;36m, line \u001b[0;32m83\u001b[0m\n\u001b[0;31m    [cite_start]model=\"gpt-4o-mini\", # Using the same model for consistency [cite: 150, 238]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkqEX-BbMZcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vizualizations"
      ],
      "metadata": {
        "id": "bQF7hKQhOSJO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wK6gun5_OUN_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}