{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpollard44/BBB_Complaints_LLM/blob/main/BBB_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code for Data scraper**"
      ],
      "metadata": {
        "id": "VFA8YV2KlYrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ],
      "metadata": {
        "id": "thGfOB41lMwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "a03a61d0-60da-42f4-ff81-c8826978fcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'undetected_chromedriver'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3994371303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mundetected_chromedriver\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0muc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'undetected_chromedriver'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to scrape random pages of BBB complaints ---\n",
        "def scrape_bbb_complaints_random(base_url, min_page=1, max_page=200, num_samples=20, pause=1.0, timeout=10):\n",
        "    \"\"\"\n",
        "    Scrape a random sample of complaint pages from BBB\n",
        "    across a range of pages to diversify by time.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Set up headless Chrome browser using undetected-chromedriver\n",
        "    options = uc.ChromeOptions()\n",
        "    options.headless = False  # Set to True if you want headless mode\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Avoid bot detection\n",
        "    options.add_argument(\n",
        "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/116.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "    driver = uc.Chrome(options=options)\n",
        "\n",
        "    # Remove webdriver detection flag from JavaScript environment\n",
        "    driver.execute_cdp_cmd(\n",
        "        \"Page.addScriptToEvaluateOnNewDocument\",\n",
        "        {\n",
        "            \"source\": \"\"\"\n",
        "                Object.defineProperty(navigator, 'webdriver', {\n",
        "                  get: () => undefined\n",
        "                });\n",
        "            \"\"\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 2) Randomly choose a set of pages to scrape\n",
        "    pages_to_scrape = sorted(random.sample(range(min_page, max_page + 1), num_samples))\n",
        "    print(f\"🔀 Randomly selected pages: {pages_to_scrape}\")\n",
        "\n",
        "    all_rows = []  # List to collect complaint data\n",
        "\n",
        "    for page in pages_to_scrape:\n",
        "        url = f\"{base_url}?page={page}\"  # Construct page URL\n",
        "        print(f\"[Page {page}] → {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        try:\n",
        "            # Wait for complaint elements to load on the page\n",
        "            WebDriverWait(driver, timeout).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"li.card.bpr-complaint-grid\"))\n",
        "            )\n",
        "        except:\n",
        "            print(f\"⚠️ No complaints found, skipping page {page}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(pause)  # Pause to ensure page is fully loaded\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")  # Parse HTML with BeautifulSoup\n",
        "        cards = soup.select(\"li.card.bpr-complaint-grid\")  # Select complaint cards\n",
        "        print(f\"  • Found {len(cards)} complaint cards\")\n",
        "\n",
        "        # 3) Loop through each complaint card and extract relevant fields\n",
        "        for card in cards:\n",
        "            date_filed = card.select_one(\"p.bpr-complaint-date span\")\n",
        "            complaint_type = card.select_one(\"div.bpr-complaint-type span\")\n",
        "            text_div = card.select_one(\"div.bpr-complaint-body > div\")\n",
        "            biz_date = card.select_one(\"p.bpr-complaint-business-response-date\")\n",
        "            biz_body = card.select_one(\"div.bpr-complaint-business-response-body\")\n",
        "            cust_date = card.select_one(\"p.bpr-customer-response-date\")\n",
        "            cust_body = card.select_one(\"div.bpr-customer-response-body\")\n",
        "            status = card.select_one(\"div.bpr-complaint-status-summary\")\n",
        "\n",
        "            # Append a dictionary of cleaned values to the output list\n",
        "            all_rows.append({\n",
        "                \"date_filed\":             date_filed.get_text(strip=True)            if date_filed else \"\",\n",
        "                \"complaint_type\":         complaint_type.get_text(strip=True)        if complaint_type else \"\",\n",
        "                \"complaint_text\":         text_div.get_text(\" \", strip=True)         if text_div else \"\",\n",
        "                \"business_response_date\": biz_date.get_text(strip=True)              if biz_date else \"\",\n",
        "                \"business_response\":      biz_body.get_text(\" \", strip=True)         if biz_body else \"\",\n",
        "                \"customer_response_date\": cust_date.get_text(strip=True)            if cust_date else \"\",\n",
        "                \"customer_response\":      cust_body.get_text(\" \", strip=True)        if cust_body else \"\",\n",
        "                \"status\":                 status.get_text(strip=True)               if status else \"\",\n",
        "                \"page\":                   page,  # Include source page number\n",
        "            })\n",
        "\n",
        "    driver.quit()  # Close browser when done\n",
        "    return pd.DataFrame(all_rows)  # Return results as a DataFrame\n"
      ],
      "metadata": {
        "id": "k23_rQ7KlMsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    BASE = \"https://www.bbb.org/us/ca/san-jose/profile/payment-processing-services/paypal-inc-1216-210387/complaints\"\n",
        "\n",
        "    # Run the scraper across 300 randomly selected pages (out of 2771)\n",
        "    df = scrape_bbb_complaints_random(BASE, min_page=1, max_page=2771, num_samples=300)\n",
        "\n",
        "    print(df.head())  # Print first few rows of the collected DataFrame\n",
        "\n",
        "    # File path for saving the scraped data\n",
        "    out_path = (\n",
        "        \"C:/Users/jthom/OneDrive/JOSH WORKING/SCHOOL WORK/\"\n",
        "        \"MS Business Analytics/GBA 6410 - Social Media Analytics and Text Mining/\"\n",
        "        \"Project/Paypal_bbb_complaints_random.csv\"\n",
        "    )\n",
        "\n",
        "    # Save the results as a CSV file\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"✅ Saved {out_path}\")"
      ],
      "metadata": {
        "id": "0Y1wBBJilMmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code for Analysis**"
      ],
      "metadata": {
        "id": "KM1XVCpLlhbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG3Hrg6Laz7J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "\n",
        "from scipy.spatial.distance import pdist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "x37TYK8TbtM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "venmo_df = pd.read_csv(\"/content/Venmo_bbb_complaints_random.csv\")\n",
        "chime_df = pd.read_csv(\"/content/Chime_bbb_complaints_random.csv\")\n",
        "paypal_df = pd.read_csv(\"/content/Paypal_bbb_complaints_random.csv\")\n",
        "square_df = pd.read_csv(\"/content/Square_bbb_complaints_random.csv\")\n",
        "\n",
        "#Add company name to df\n",
        "venmo_df['Company'] = 'Venmo'\n",
        "chime_df['Company'] = 'Chime'\n",
        "paypal_df['Company'] = 'Paypal'\n",
        "square_df['Company'] = 'Square'\n",
        "\n",
        "#combine datasets from all companies\n",
        "df_complete = pd.concat([venmo_df, chime_df, paypal_df, square_df], ignore_index=True)\n",
        "df_complete.to_csv(\"/content/Complete_BBB_Corpus.csv\")\n"
      ],
      "metadata": {
        "id": "467PkWOEa8Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-Processing"
      ],
      "metadata": {
        "id": "1C8dgwtzwWgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text data preprocessing\n",
        "def preprocessing(data):\n",
        "\n",
        "    #convert all to string and convert to lowercase\n",
        "    data['complaint_text'] = data['complaint_text'].fillna('').astype(str).str.lower()\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean = []\n",
        "\n",
        "    for message in data['complaint_text']:\n",
        "\n",
        "        #remove punctuation\n",
        "        message = re.sub(r'[^\\w\\s]', '', message)\n",
        "\n",
        "        #tokenization\n",
        "        tokens = word_tokenize(message)\n",
        "\n",
        "        #Remove Stop words\n",
        "        filtered = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        #Remove lemma\n",
        "        filtered = [lemmatizer.lemmatize(word) for word in filtered]\n",
        "\n",
        "        clean.append(filtered)\n",
        "\n",
        "    return clean"
      ],
      "metadata": {
        "id": "pNrTwzQda-tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorization"
      ],
      "metadata": {
        "id": "QSBCwOXrwgaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorization(normalized_data):\n",
        "    # Join tokens into space-separated strings\n",
        "    joined_data = [' '.join(tokens) for tokens in normalized_data]\n",
        "\n",
        "    tv = TfidfVectorizer(min_df=0.10, max_df=0.95, norm='l2',\n",
        "                         use_idf=True, smooth_idf=True)\n",
        "\n",
        "    tv_matrix = tv.fit_transform(joined_data) # Fit the vectorizer on the text data and transform it into a TF-IDF matrix\n",
        "    tv_matrix = tv_matrix.toarray() # Convert the resulting sparse matrix to a dense NumPy array\n",
        "    vocab = tv.get_feature_names_out() # Extract the feature names (i.e., vocabulary terms)\n",
        "\n",
        "    # Create a DataFrame from the TF-IDF array for easier manipulation\n",
        "    vectorized = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
        "\n",
        "    return vectorized # Return the vectorized text data as a DataFrame\n",
        "\n",
        "normalized_data = preprocessing(df_complete)\n",
        "joined_texts = [' '.join(tokens) for tokens in normalized_data]\n",
        "\n",
        "# Call the function to create the TF-IDF feature matrix\n",
        "vectorized = vectorization(normalized_data)"
      ],
      "metadata": {
        "id": "7C6Trg7zbAzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the pairwise cosine similarity between all complaint vectors\n",
        "cosine_sim_matrix = pd.DataFrame(cosine_similarity(vectorized))\n",
        "\n",
        "# Extract the underlying NumPy array from the TF-IDF DataFrame\n",
        "tfidf_array = vectorized.values\n",
        "\n",
        "# Calculate the pairwise (condensed) Euclidean distances between all complaints\n",
        "condensed_euclidean_dist = pdist(tfidf_array, metric='euclidean')\n",
        "\n",
        "# Perform hierarchical clustering using Ward's linkage method\n",
        "Z = linkage(condensed_euclidean_dist, method='ward')"
      ],
      "metadata": {
        "id": "5Rev1kGfbCzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster and Vizualization"
      ],
      "metadata": {
        "id": "fsZCME70wkcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to Perform Clustering and Visualize with 2D PCA ---\n",
        "def cluster_and_visualize_2D(tfidf_array, Z, num_clusters, company_labels):\n",
        "\n",
        "    # Step 1: Assign cluster labels\n",
        "    cluster_labels = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
        "\n",
        "    # Step 2: PCA (2 Components)\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_data = pca.fit_transform(tfidf_array)\n",
        "\n",
        "    # Step 3: Build DataFrame for plotting\n",
        "    cluster_df = pd.DataFrame({\n",
        "        'PCA1': reduced_data[:, 0],\n",
        "        'PCA2': reduced_data[:, 1],\n",
        "        'Cluster': cluster_labels,\n",
        "        'Company': company_labels\n",
        "    })\n",
        "\n",
        "    # Step 4: 2D Plot — color by cluster, shape by company\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(\n",
        "        data=cluster_df,\n",
        "        x='PCA1', y='PCA2',\n",
        "        hue='Cluster',\n",
        "        style='Company',\n",
        "        palette='tab10',\n",
        "        s=70,\n",
        "        alpha=0.8,\n",
        "        edgecolor='k'\n",
        "    )\n",
        "\n",
        "    plt.title(f'2D PCA Projection of Complaint Clusters (k={num_clusters})')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Cluster / Company\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return cluster_df\n",
        "\n",
        "# --- Call the function ---\n",
        "num_clusters = 4\n",
        "cluster_results_2d = cluster_and_visualize_2D(tfidf_array, Z, num_clusters, df_complete['Company'])"
      ],
      "metadata": {
        "id": "GR-hH-iym2Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized['Cluster'] = cluster_results_2d['Cluster'].values  # Add cluster labels to TF-IDF matrix"
      ],
      "metadata": {
        "id": "f21d1E-Tm07P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### n grams"
      ],
      "metadata": {
        "id": "CNgsec_MwoRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_ngrams_by_cluster(texts, cluster_labels, n=2, top_k=10):\n",
        "    df = pd.DataFrame({'text': texts, 'cluster': cluster_labels})\n",
        "    cluster_ngrams = {}\n",
        "\n",
        "    for cluster_id in sorted(df['cluster'].unique()):\n",
        "        print(f\"\\n🔍 Top {n}-grams for Cluster {cluster_id}\")\n",
        "        cluster_texts = df[df['cluster'] == cluster_id]['text']\n",
        "\n",
        "        vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
        "        X = vectorizer.fit_transform(cluster_texts)\n",
        "        ngram_freq = X.sum(axis=0)\n",
        "\n",
        "        ngram_counts = [(ngram, int(ngram_freq[0, idx])) for ngram, idx in vectorizer.vocabulary_.items()]\n",
        "        top_ngrams = sorted(ngram_counts, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "        cluster_ngrams[cluster_id] = top_ngrams\n",
        "\n",
        "        for phrase, count in top_ngrams:\n",
        "            print(f\"{phrase}: {count}\")\n",
        "\n",
        "    return cluster_ngrams\n"
      ],
      "metadata": {
        "id": "EM43Ds1FAQqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ngrams_histograms_by_cluster(ngram_data):\n",
        "    clusters = sorted(ngram_data.keys())\n",
        "    num_clusters = len(clusters)\n",
        "\n",
        "    # Setup for a 2x2 grid\n",
        "    rows, cols = 2, 2\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(12, 10), constrained_layout=True)\n",
        "    axes = axes.flatten()  # make it easy to index\n",
        "\n",
        "    for i, cluster_id in enumerate(clusters):\n",
        "        top_ngrams = ngram_data[cluster_id]\n",
        "        phrases, counts = zip(*top_ngrams)\n",
        "\n",
        "        axes[i].barh(phrases[::-1], counts[::-1], color='skyblue')\n",
        "        axes[i].set_title(f\"Cluster {cluster_id}\")\n",
        "        axes[i].set_xlabel(\"Frequency\")\n",
        "        axes[i].set_ylabel(\"Top N-grams\")\n",
        "\n",
        "    # Hide any unused subplots (if < 4 clusters)\n",
        "    for j in range(len(clusters), len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.suptitle(\"Top N-grams by Cluster (2x2 Layout)\", fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MpayH6BRvpUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and return top n-grams\n",
        "ngram_data = top_ngrams_by_cluster(joined_texts, vectorized['Cluster'], n=2, top_k=10)\n",
        "\n",
        "# Plot using already extracted data\n",
        "plot_ngrams_histograms_by_cluster(ngram_data)\n"
      ],
      "metadata": {
        "id": "39zZcp0cwSRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for Severity_Score using OpenAI API (1hour to label 1000)"
      ],
      "metadata": {
        "id": "1amf4sFHSuPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import openai\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import csv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key from Colab Secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('JP_OPEN_AI_Key')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Check if API key is loaded\n",
        "if not api_key:\n",
        "    print(\"Error: No API key found. Please set JP_OPEN_AI_Key in Colab Secrets.\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize Open AI client\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Read the dataset with robust parsing\n",
        "try:\n",
        "    df = pd.read_csv(\"Complete_BBB_Corpus.csv\", quoting=csv.QUOTE_ALL, engine='python', on_bad_lines='skip')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Input file 'Complete_BBB_Corpus.csv' not found in the same directory.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error reading CSV: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Sample 1000 records or all if fewer than 1000\n",
        "sample_size = min(1000, len(df))\n",
        "try:\n",
        "    sampled_df = df.sample(n=sample_size, random_state=42)\n",
        "except ValueError as e:\n",
        "    print(f\"Error sampling data: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Function to generate sensitivity score using Open AI API\n",
        "def get_sensitivity_score(complaint_text):\n",
        "    if not isinstance(complaint_text, str):\n",
        "        return None, \"Error: Complaint text is missing or invalid.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a fraud and customer complaint analyst.\n",
        "    Evaluate the following customer complaint and assign a severity score between 0.0 and 1.0, reflecting the overall seriousness and business impact of the issue. This score should guide how urgently a company should respond or escalate the complaint.\n",
        "    Consider the following six factors:\n",
        "    Financial impact (e.g., significant losses, refunds, overcharges)\n",
        "    Emotional sensitivity (e.g., tone, distress, hardship expressed)\n",
        "    Fraud or security concerns (e.g., unauthorized transactions, identity theft)\n",
        "    Urgency (e.g., impact on essentials like rent, food, time-sensitive problems)\n",
        "    Customer service breakdown (e.g., repeated failures, no resolution offered)\n",
        "    Tone or language (e.g., legal threats, accusations, emotionally charged language)\n",
        "    Score guidelines:\n",
        "    0.0–0.2: Minor — low impact, no urgency, easily resolvable\n",
        "    0.3–0.5: Moderate — needs attention but not urgent or damaging\n",
        "    0.6–0.8: High — serious issue, business should respond quickly\n",
        "    0.9–1.0: Critical — urgent, high-risk to customer or company (legal, financial, PR)\n",
        "    Provide the severity score as a number and a brief rationale (1-3 sentences) explaining your reason for given that score.\n",
        "    Complaint text: \"{complaint_text}\"\n",
        "    Response format:\n",
        "    Score: <number>\n",
        "    Rationale: <explanation>\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a fraud and customer complaint analyst.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        result = response.choices[0].message.content.strip()\n",
        "        score_line = result.split(\"\\n\")[0]\n",
        "        rationale_line = result.split(\"\\n\")[1]\n",
        "        score = float(score_line.replace(\"Score: \", \"\"))\n",
        "        rationale = rationale_line.replace(\"Rationale: \", \"\")\n",
        "        return score, rationale\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing complaint: {e}\")\n",
        "        return None, f\"API error: {str(e)}\"\n",
        "\n",
        "# Process sampled complaints and collect results\n",
        "results = []\n",
        "for index, row in sampled_df.iterrows():\n",
        "    complaint_text = row['complaint_text']\n",
        "    row_id = index\n",
        "    complaint_id = row.get('complaint_id', str(uuid.uuid4()))\n",
        "\n",
        "    print(f\"Processing row_id: {row_id}, complaint_id: {complaint_id}\")\n",
        "    score, rationale = get_sensitivity_score(complaint_text)\n",
        "\n",
        "    results.append({\n",
        "        'row_id': row_id,\n",
        "        'complaint_id': complaint_id,\n",
        "        'sensitivity_score': score,\n",
        "        'rationale': rationale\n",
        "    })\n",
        "\n",
        "    time.sleep(1)  # Avoid rate limiting\n",
        "\n",
        "# Create DataFrame from results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save to CSV\n",
        "try:\n",
        "    results_df.to_csv(\"openai_sensitivity_scores.csv\", index=False)\n",
        "    print(f\"Output saved to 'openai_sensitivity_scores.csv' with {len(results_df)} records\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving output: {e}\")\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "hwzmhw0_6v2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "s5LeDvzwXjCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forrest"
      ],
      "metadata": {
        "id": "ajeJE3ruXwkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "# Load and merge files for training\n",
        "def load_and_merge_files(file1_path, file2_path, id_column='id', score_column='sensitivity_score'):\n",
        "    try:\n",
        "        df1 = pd.read_csv(file1_path, quoting=csv.QUOTE_ALL, engine='python', on_bad_lines='skip')\n",
        "        df2 = pd.read_csv(file2_path, quoting=csv.QUOTE_ALL, engine='python', on_bad_lines='skip')\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File not found - {e}\")\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    # Merge on the specified ID column\n",
        "    merged_df = pd.merge(df1, df2, on=id_column, how='inner')\n",
        "\n",
        "    # Map score to label\n",
        "    def map_score_to_label(score):\n",
        "        if 0.0 <= score <= 0.2:\n",
        "            return 'Minor'\n",
        "        elif 0.3 <= score <= 0.6:\n",
        "            return 'Moderate'\n",
        "        elif 0.7 <= score <= 0.8:\n",
        "            return 'High'\n",
        "        elif 0.9 <= score <= 1.0:\n",
        "            return 'Critical'\n",
        "        else:\n",
        "            return 'Invalid'\n",
        "\n",
        "    merged_df['sensitivity_label'] = merged_df[score_column].apply(map_score_to_label)\n",
        "    return merged_df, df1  # Return merged training data and full file_1\n",
        "\n",
        "# Train Random Forest model\n",
        "def train_random_forest_model(df, text_column='complaint_text', label_column='sensitivity_label'):\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "    df[text_column] = df[text_column].apply(clean_text)\n",
        "\n",
        "    # Remove labels with fewer than 2 samples\n",
        "    label_counts = df[label_column].value_counts()\n",
        "    valid_labels = label_counts[label_counts >= 2].index\n",
        "    df = df[df[label_column].isin(valid_labels)]\n",
        "\n",
        "    print(\"Filtered label counts:\\n\", df[label_column].value_counts())\n",
        "\n",
        "    # Stratified split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df[text_column], df[label_column],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df[label_column]\n",
        "    )\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train Random Forest\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    print(\"Classification Report on Training Data:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model, vectorizer\n",
        "\n",
        "# Predict labels on full dataset\n",
        "def predict_labels(full_df, model, vectorizer, text_column='complaint_text'):\n",
        "    if text_column not in full_df.columns:\n",
        "        print(f\"Error: Column '{text_column}' not found in the dataset.\")\n",
        "        return None\n",
        "\n",
        "    full_df['cleaned_text'] = full_df[text_column].apply(clean_text)\n",
        "    X_full = vectorizer.transform(full_df['cleaned_text'])\n",
        "    full_df['predicted_sensitivity_label'] = model.predict(X_full)\n",
        "    return full_df\n",
        "\n",
        "# File paths\n",
        "file_1 = \"Complete_BBB_Corpus.csv\"  # Full dataset\n",
        "file_2 = \"openai_sensitivity_scores2.csv\"  # Training data with sensitivity scores\n",
        "\n",
        "# Load and merge for training, get full file_1\n",
        "train_df, full_df = load_and_merge_files(file_1, file_2)\n",
        "\n",
        "# Train the model\n",
        "model, vectorizer = train_random_forest_model(train_df)\n",
        "\n",
        "# Save model and vectorizer\n",
        "joblib.dump(model, 'rf_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "print(\"Model and vectorizer saved as 'rf_model.pkl' and 'tfidf_vectorizer.pkl'\")\n",
        "\n",
        "# Predict on full file_1 dataset\n",
        "result_full_df = predict_labels(full_df, model, vectorizer)\n",
        "\n",
        "# Save or display results\n",
        "if result_full_df is not None:\n",
        "    output_path = \"predicted_full_complaints.csv\"\n",
        "    result_full_df.to_csv(output_path, index=False)\n",
        "    print(f\"Predictions saved to {output_path}\")\n",
        "    print(\"\\nSample of predictions:\")\n",
        "    print(result_full_df[['complaint_text', 'predicted_sensitivity_label']].head())\n",
        "else:\n",
        "    print(\"Failed to generate predictions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BIWsK7e-51u",
        "outputId": "de72863a-7242-4b14-bcce-e4720782f06a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered label counts:\n",
            " sensitivity_label\n",
            "High        490\n",
            "Critical    396\n",
            "Moderate    113\n",
            "Name: count, dtype: int64\n",
            "Classification Report on Training Data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Critical       0.68      0.57      0.62        79\n",
            "        High       0.59      0.80      0.68        98\n",
            "    Moderate       1.00      0.04      0.08        23\n",
            "\n",
            "    accuracy                           0.62       200\n",
            "   macro avg       0.76      0.47      0.46       200\n",
            "weighted avg       0.67      0.62      0.59       200\n",
            "\n",
            "Model and vectorizer saved as 'rf_model.pkl' and 'tfidf_vectorizer.pkl'\n",
            "Predictions saved to predicted_full_complaints.csv\n",
            "\n",
            "Sample of predictions:\n",
            "                                      complaint_text  \\\n",
            "0                                  Suspended account   \n",
            "1  Subject: Urgent: Fraudulent Activity on My Ven...   \n",
            "2  I contacted Venmo / Paypal customer support de...   \n",
            "3  I sold some test strips to Two Mom's Buy Test ...   \n",
            "4  I just found out my card was taken and has bee...   \n",
            "\n",
            "  predicted_sensitivity_label  \n",
            "0                        High  \n",
            "1                    Critical  \n",
            "2                    Critical  \n",
            "3                    Moderate  \n",
            "4                    Critical  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "FTnA38H5XqKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_logistic_regression_model(df, text_column='complaint_text', label_column='sensitivity_label'):\n",
        "    # Drop missing\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "\n",
        "    # Clean the text\n",
        "    df[text_column] = df[text_column].apply(clean_text)\n",
        "\n",
        "    # Remove labels with fewer than 2 samples\n",
        "    label_counts = df[label_column].value_counts()\n",
        "    valid_labels = label_counts[label_counts >= 2].index\n",
        "    df = df[df[label_column].isin(valid_labels)]\n",
        "\n",
        "    # Report final label distribution\n",
        "    print(\"Filtered label counts:\\n\", df[label_column].value_counts())\n",
        "\n",
        "    # Stratified split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df[text_column], df[label_column],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df[label_column]\n",
        "    )\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train Logistic Regression\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model, vectorizer\n",
        "\n",
        "\n",
        "logreg_model, logreg_vectorizer = train_logistic_regression_model(result_df)\n"
      ],
      "metadata": {
        "id": "rFMJIKrmeFIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e36a0c-c483-455a-bc34-c045d25a81d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered label counts:\n",
            " sensitivity_label\n",
            "High        490\n",
            "Critical    396\n",
            "Moderate    113\n",
            "Name: count, dtype: int64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Critical       0.59      0.52      0.55        79\n",
            "        High       0.55      0.73      0.63        98\n",
            "    Moderate       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.56       200\n",
            "   macro avg       0.38      0.42      0.39       200\n",
            "weighted avg       0.50      0.56      0.53       200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert"
      ],
      "metadata": {
        "id": "vOzDT16cXsYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def train_bert_model(df, text_column='complaint_text', label_column='sensitivity_label'):\n",
        "    # Drop missing values\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "\n",
        "    # Filter labels with at least 2 samples\n",
        "    df = df[df[label_column].map(df[label_column].value_counts()) >= 2]\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    df['label'] = le.fit_transform(df[label_column])\n",
        "\n",
        "    # Stratified split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_df, test_df = train_test_split(\n",
        "        df, test_size=0.2, random_state=42, stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Convert to Hugging Face Dataset\n",
        "    train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "    test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "    dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Tokenization function\n",
        "    def tokenize_function(example):\n",
        "        return tokenizer(\n",
        "            example[text_column],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        )\n",
        "\n",
        "    # Apply tokenization\n",
        "    dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Rename label column for Hugging Face Trainer\n",
        "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "    # Load model\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=len(le.classes_)\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    logging_dir=\"./logs\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test']\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    predictions = trainer.predict(dataset['test'])\n",
        "    y_pred = predictions.predictions.argmax(axis=1)\n",
        "    y_true = predictions.label_ids\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "\n",
        "    return model, tokenizer, le\n",
        "\n"
      ],
      "metadata": {
        "id": "TcOA3j7_euUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model, bert_tokenizer, label_encoder = train_bert_model(result_df)"
      ],
      "metadata": {
        "id": "fifPQ8-KeyJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Examples"
      ],
      "metadata": {
        "id": "KA2Bw2ivQx-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Critical Example - Complaint #48:\n",
        "\n",
        "on november 17th 2023 i was scammed using venmo  within a minute i realized i was scammed called my bank to stop payment and called venmo to stop payment   they all have record of my call   venmos policy states that this was a friend to friend exchange and they continued to front the 75000    i contacted venmo and followed their directions to file a police report which i did   since then venmo will not close my account as they say i have a negative balance   when someone accidently venmos me money it is just taken off the debt they say i owe   i am only asking venmo to close my account so this doesnt happen any more to my friends and family that think they can pay me by sending money via venmo    i did not lose any of my money as the bank stopped payment on 111723   however i will continue to lose money if they wont close my account"
      ],
      "metadata": {
        "id": "OpWLojNWQrtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low Example - Complaint #9\n",
        "i have been emailing venmo to unfreeze my account they said theyre going to hold my money for 180 days i just want to close that account and get my money back im really frustrated"
      ],
      "metadata": {
        "id": "mOlqj42FQw9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heat Map of Severity Score"
      ],
      "metadata": {
        "id": "KLfU3RavPren"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the merged_severity.csv file\n",
        "try:\n",
        "    df = pd.read_csv(\"merged_severity.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Input file 'merged_severity.csv' not found.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error reading CSV: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "\n",
        "# Define a function to create a pivot table with optional percentage calculation\n",
        "def create_pivot_table(df, index_col, column_col, value_col='id', as_percentage=False):\n",
        "    if index_col not in df.columns or column_col not in df.columns:\n",
        "        print(f\"Error: One or both of {index_col}, {column_col} not found in DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    # Create the pivot table with counts\n",
        "    pivot = df.groupby([index_col, column_col]).size().unstack(fill_value=0)\n",
        "\n",
        "    if as_percentage:\n",
        "        # Calculate percentages across the column_col\n",
        "        pivot = pivot.div(pivot.sum(axis=0), axis=1) * 100\n",
        "        pivot = pivot.round(2)  # Round to 2 decimal places\n",
        "    else:\n",
        "        print(f\"\\nCount of {index_col} per {column_col}:\")\n",
        "\n",
        "    return pivot\n",
        "\n",
        "# Define a function to calculate percentages from a pivoted DataFrame (for already pivoted data)\n",
        "def calculate_percentages(df, index_col='sensitivity_label'):\n",
        "    # Handle case where index is unnamed\n",
        "    index_name = df.index.name if df.index.name is not None else None\n",
        "    if index_name != index_col and index_col in df.columns:\n",
        "        df = df.set_index(index_col)\n",
        "    elif index_name != index_col and index_col not in df.columns:\n",
        "        print(f\"Error: {index_col} not found in DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    # Calculate percentages across each company\n",
        "    percentage_table = df.div(df.sum(axis=0), axis=1) * 100\n",
        "    percentage_table = percentage_table.round(2)  # Round to 2 decimal places\n",
        "\n",
        "    return percentage_table\n",
        "\n",
        "\n",
        "\n",
        "# Generate the percentage pivot table for sensitivity_label by Company\n",
        "pivot_table = create_pivot_table(df, index_col='sensitivity_label', column_col='Company', as_percentage=True)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "if pivot_table is not None:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(\n",
        "        pivot_table,\n",
        "        annot=True,  # Show percentage values in cells\n",
        "        fmt='.2f',   # Format numbers to 2 decimal places\n",
        "        cmap='YlOrRd',  # Color scheme (Yellow-Orange-Red)\n",
        "        cbar_kws={'label': 'Percentage (%)'}\n",
        "    )\n",
        "    plt.title('Percentage of Sensitivity Labels by Company')\n",
        "    plt.xlabel('Company')\n",
        "    plt.ylabel('Sensitivity Label')\n",
        "\n",
        "    # Save the heatmap to a file\n",
        "    # plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    # print(\"\\nHeatmap saved as 'heatmap.png'\")\n",
        "\n",
        "    # Display the plot (optional, depending on environment)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GQHfoAP9Pq1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Critical Complaints Analysis"
      ],
      "metadata": {
        "id": "IL2Y4NH7xK7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic for Identifying Department"
      ],
      "metadata": {
        "id": "9fzMlkibxW2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install bertopic\n",
        "# 1. Install necessary packages if not already installed\n",
        "# !pip install bertopic sentence-transformers openpyxl nltk\n",
        "\n",
        "# 2. Import libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# 3. Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 4. Load your Excel file\n",
        "df = pd.read_csv(\"merged_severity.csv\")\n",
        "\n",
        "# 5. Filter to only include 'Critical' complaints (sensitivity_score ≥ 0.9)\n",
        "critical_df = df[df['sensitivity_score'] >= 0.9].copy()\n",
        "\n",
        "# 6. Drop missing or empty complaints\n",
        "critical_df = critical_df.dropna(subset=['complaint_text'])\n",
        "critical_df = critical_df[critical_df['complaint_text'].str.strip() != \"\"]\n",
        "\n",
        "# 7. Define preprocessing function with keyword removal\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove target keywords\n",
        "    keywords_to_remove = [\"paypal\", \"chime\", \"venmo\", \"square\", \"cashapp\"]\n",
        "    for keyword in keywords_to_remove:\n",
        "        text = text.replace(keyword, \"\")\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "\n",
        "    # Tokenize\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Rejoin into string\n",
        "    return \" \".join(words)\n",
        "\n",
        "# 8. Apply preprocessing\n",
        "critical_df[\"cleaned_complaint\"] = critical_df[\"complaint_text\"].apply(preprocess_text)\n",
        "\n",
        "# 9. Convert text column to list\n",
        "complaints = critical_df[\"cleaned_complaint\"].tolist()\n",
        "\n",
        "# 10. Load sentence embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 11. Initialize BERTopic\n",
        "topic_model = BERTopic(embedding_model=embedding_model, verbose=True)\n",
        "\n",
        "# 12. Fit the model to the complaints\n",
        "topics, probs = topic_model.fit_transform(complaints)\n",
        "\n",
        "# Replace your model with the reduced version\n",
        "topic_model = topic_model.reduce_topics(complaints, nr_topics=10)\n",
        "\n",
        "# 13. Add results back to your DataFrame\n",
        "critical_df[\"topic\"] = topics\n",
        "critical_df[\"probability\"] = probs\n",
        "\n",
        "# 14. Save clustered results (optional)\n",
        "critical_df.to_excel(\"critical_complaints_clustered.xlsx\", index=False)\n",
        "\n",
        "# 15. View top topics\n",
        "print(topic_model.get_topic_info().head())\n",
        "\n",
        "\n",
        "# 13. Visualize topics (in notebook)\n",
        "topic_model.visualize_barchart(top_n_topics=10)"
      ],
      "metadata": {
        "id": "Q4iaK5GweTC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Visualize topics (in notebook)\n",
        "topic_model.visualize_topics()"
      ],
      "metadata": {
        "id": "eZGS6uHYeV7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See top keywords for each topic\n",
        "for topic_id in critical_df['topic'].unique():\n",
        "    print(f\"\\nTopic {topic_id}:\")\n",
        "    print(topic_model.get_topic(topic_id))\n"
      ],
      "metadata": {
        "id": "wVAvN5L6eamT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Mapping"
      ],
      "metadata": {
        "id": "G1Pl64AmxnRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define topic label mapping\n",
        "topic_labels = {\n",
        "    -1: \"General Complaints / Miscellaneous\",\n",
        "     0: \"Account Closures & Withheld Funds\",\n",
        "     1: \"Disputed Transactions & Denied Claims\",\n",
        "     2: \"Held Business Funds & Delayed Releases\",\n",
        "     3: \"Hacked Accounts & Stolen Funds\",\n",
        "     4: \"Identity Verification & Account Setup Issues\",\n",
        "     5: \"Scams Involving Delivery or Sellers\",\n",
        "     6: \"Refund Disputes & Fraud Claims\"\n",
        "}\n",
        "\n",
        "# 2. Apply the mapping to your BERTopic-labeled DataFrame\n",
        "critical_df[\"topic_label\"] = critical_df[\"topic\"].map(topic_labels)\n",
        "\n",
        "# 3. Preview the mapping (optional)\n",
        "print(critical_df[[\"topic\", \"topic_label\"]].drop_duplicates().sort_values(\"topic\"))\n",
        "\n",
        "# 4. (Optional) Save updated file\n",
        "critical_df.to_excel(\"critical_complaints_labeled.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "kjD68KZmeb8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_counts = critical_df[\"topic_label\"].value_counts()\n",
        "print(\"\\nComplaint counts by topic:\")\n",
        "print(topic_counts)\n"
      ],
      "metadata": {
        "id": "lekTxei9edX-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}